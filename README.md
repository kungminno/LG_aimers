# MQL 데이터 기반 B2B 영업 기회 창출 예측 모델

## 📌 프로젝트 개요

- **주제:** MQL 데이터 기반 B2B 영업 기회 창출 예측 모델 개발  
- **목적:** 영업 효율성을 높이고, 잠재 고객을 정확하게 식별하여 마케팅 및 영업 전략 최적화  
- **모델:** BERT base multilingual  
- **성과:** 해커톤 본선 진출(22위/843팀)
---

## 📌 문제 정의 및 도전 과제  

- **Public vs Private 데이터 구분:**  
  - 사용하는 데이터와 실제 점수를 측정하는 데이터는 공개(Public)와 비공개(Private)로 나뉘어 실 점수를 직접 측정할 수 없음.  
- **데이터 불균형 문제:**  
  - 예측 대상인 'is_converted' 값의 True:False 비율은 약 1:10 (True = 4,834, False = 49,943).  
- **일반화 능력 중요성:**  
  - 비공개 데이터에서의 성능을 고려하여 과적합 방지와 일반화 성능 최적화에 집중.  

---

## 📌 모델 선정  

### **선정 기준**  

- 범주형 및 숫자형 데이터 처리  
- 다국어 지원 및 확장성  
- 대규모 사전 학습 데이터 활용  

### **선택 모델: BERT base multilingual**  

- 가볍고 효율적인 구조  
- 범주형 및 숫자형 데이터 처리 능력  
- 102개 언어로 사전 학습된 다국어 모델  
- 대규모 Wikipedia 데이터 활용  

---

## 📌 데이터 전처리 및 분석  

1. **데이터 불균형 해소**  
    - 편항된 True와 False 데이터의 차이를 줄이기 위해 True 데이터를 **8배 증강**  
    - **Easy Data Augmentation** 적용  
      - 행(row)과 열(column)을 랜덤하게 선택하여 **NaN 삽입(비율 0.2 적용)**  
      - 삽입된 데이터를 기존 데이터와 결합하여 증강 완료  

2. **특성 중요도 분석 및 불필요한 변수 제거**  
    - 랜덤 포레스트로 평가된 변수 중요도가 가장 낮은 5가지 column 삭제  

3. **결측치 처리**  
    - NaN을 **'nan' 문자열로 변환**  
    - **이유:**  
      - 데이터 부족으로 삭제가 어려움  
      - 대체값 부적합성  
      - 결측 자체가 정보로 활용 가능성 보임  

4. **불용어 제거**  
    - ‘/’와 같은 특수 문자 및 의미 없는 불용어 제거  

5. **BERT 입력 데이터 변환**  
    - 모든 column을 **‘col명_데이터’** 형식으로 병합하여 하나의 문자열로 구성 → BERT 모델 입력 형식으로 변환  

---

## 📌 모델 설계 및 학습  

### **일반화 성능 및 과적합 방지 기법**  

- **SAM (Sharpness Aware Minimization):** 평탄한 손실 함수 최적화로 일반화 성능 향상  
- **Weight Decay:** 과적합 방지  
- **Early Stopping:** 검증 손실 기준 학습 중단  
- **Dropout:** 일부 뉴런 비활성화  

---

## 📌 학습 과정 및 결과  

### **훈련 및 검증**  

- **전처리 데이터 기반 훈련:**  
  - **AdamW Optimizer** 적용  
  - **SAM 기반 두 단계 학습** 적용  
  - **Early Stopping 및 Checkpoint 저장** 적용  

### **추가 학습 (Fine-tuning)**  

- True 데이터 기반 **세부 튜닝** 적용  
- 편향 해소 및 성능 향상 확인  

---

## 📌 결과 분석 및 성과  

### **Public 데이터 기준**  

- **0.7693점** → 상위 **2.8% (24위/843팀)**  
- **상위권 점수:**  
  - **1위:** 0.7959  
  - **2위:** 0.7843  
  - **3위:** 0.7843  

### **Private 데이터 기준**  

- **0.7876점** → 상위 **2.6% (22위/843팀)**  
- **Public 데이터보다 순위 상승(24위 → 22위)** 달성  
- **상위권 점수:**  
  - **1위:** 0.8091  
  - **2위:** 0.8085  
  - **3위:** 0.8073  

---

## 📌 결론

- 해커톤 대회 본선 진출 및 최종 **상위 22위 기록**  
- **일반화 성능 향상 전략(SAM)** 과 **데이터 증강 기법**의 성공적인 적용  

---

## 📌 사용 기술  

- **프로그래밍 언어:** Python  
- **딥러닝 프레임워크:** PyTorch, HuggingFace Transformers
- **최적화 기법:** SAM(Sharpness-Aware Minimization), AdamW, Early Stopping, Dropout, Weight Decay  
